\ifpdf
    \graphicspath{{figures/}{figures/comparisons}}
\else
    \graphicspath{{figures/}{figures/comparisons}}
\fi

%: ----------------------- name of chapter  -------------------------
\chapter{Implementation} % top level followed by section, subsection
This chapter describes choosen implementation environment. This chapter also describes Android application, which was created in order to acquire image sequences with additional sensor data. Sturctures of both Android and Desktop projects are explained and most important implementation details of proposed algorithms and strategies are discussed.
\section{Choosing Environment}
Currently Android has one of the best API, which allows programmers to acquire Sensor fusioned data of rotation and linear acceleration.
In order to avoid coding from scratch all epipolar geometry algorithms C++ version of OpenCV library was used. Image acquiering process was seperated from model reconstruction. This allowed to saved a lot of time in debbuging, which is extremly hard in native C++ development on Android. In order to enrich images with sensor data Android app was created and standalone C++ desktop app to perform processing and evaluation.
\section{Project Structure} \label{sec:ProjectStructure}
Both applications source codes can be found on attached CD and under the Github webpage: TODO \url{https://github.com/KrzysztofWrobel/MasterThesisSource.git}. In general project was seperated to different subprojects: Android and Desktop. In addition some of sample datasets were added to Dataset folder in order to perform similar to described in next chapter evaluations.
\section{"Sensor Enhanced Images Camera" - Android Gradle based project}
In order to capture images and associate them with Sensor data custom photo capture app called "Sensor Enhanced Images Camera" was created.
Using this application camera orientation angles can be tracked in real-time in reference to Earth's coordinate system. Also linear acceleration, current velocity and relative translation from last taken picture location can be tracked. 
When taking picture current camera rotation, relative translation are stored in custom JSON file. Corresponding image is saved along this JSON file in folder, which is created on every app start-up. It allows for taking different dataset captures without mixing them up.
\subsection{Installation}
In order to compile and distribute Android application Gradle based built system was used. This is currently recomended way to maintain Android based projects[TODO ref. google]. Currently this app supports only devices with Android 4.0 and above. In order to compile this project it's recomended to install Android Studio, which already contains necessery SDK and also has built-in Gradle support. More informations about configuration, compilation and install steps can be found in README.md in main catalog of attached source code. 
\subsection{User Interface}
As mentioned earlier apps allows to track all necessery external camera parameters in real-time. This is how first version of user interface look like:
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{AndroidScreenShot}
    \caption{}
    \label{fig:AndroidScreenShot}
\end{figure}
\clearpage
In order to capture photo, user only has to click in the screen center. No additional configuration is required. To use use acquired datasets in other program smartphone has to be connected to the computer. Folders are created with actual time suffixes in main catalog of internal SD card.
\subsection{Important Implementation Aspects}
There were two important aspects of Andoid application implementation:
\begin{enumerate}
\item Camera rotation calculation
\item Heuristcal movement estimation
\end{enumerate}
\subsubsection{Rotation calculation}
Android SDK has already built-in API, which can be used to access sensor fusioned data. In particular for rotation measurment acquisition $Sensor.TYPE\_ROTATION\_VECTOR$ was used. It returns 9-degree quaternion in reference to geomagnetic north. In order to decompose it to euler angles helper method was used:
\lstset{escapechar=@,style=customjava}
\begin{lstlisting}
private float[] euclidanAnglesFromQuaternion(float[] quaternion) {
        ...
        //Converts quaternion to 4x4 rotation matrix
        SensorManager.getRotationMatrixFromVector(rotMat, quaternion);
        ...
        //Extracts euclidan Angles in radians
        SensorManager.getOrientation(rotMat, orientation);
        ...
        return orientation;
}
\end{lstlisting}
\subsubsection{Custom heouristic for move estimation}
Using currently available Android sensor data it is hard to estimate relative translation of the device. Linear accelaration measurments, which can be accessed with $Sensor.TYPE\_LINEAR\_ACCELERATION$ constant, are very noisy. Such noisy data with double intergration over time results in quite big errors after some time. First integration is required to acquire current velocity change. Second integration over velocity is required to calculate position change in particular moment.
However using human-walking model description it can be improved with following heuristical constraints over calculated velocity:
\begin{enumerate}
\item When new linear acceleration sensor data is ready, first apply lowPass filtering in order to reduce some high frequency noise.
\item Change sensor datda vector from local camera coordinates to global reference system(multiply with inverted rotation matrix)
\item Decide depending on current state, if deviceMovmentState has changed:
\begin{enumerate}
\item If device was previously IDLE and incoming Acceleration value is bigger than $0.5\frac{m}{s^2}$ change device state to MOVING and reset current velocity to $0\frac{m}{s}$
\item If device was previously MOVING and incoming Acceleration value is smaller than $0.1\frac{m}{s^2}$ change device state to IDLE
\end{enumerate}
\item Only if device is currently moving:
\begin{enumerate}
\item Update device velocity
\item Check current speed with walk constraint(People walk with avarage speed $1.5\frac{m}{s}$) and eventually scale it down to maximum value
\item Update device postion
\end{enumerate}
\end{enumerate}
Code snippet of described movement estimation heuristic was derived from Android application and can be found in listing \ref{lst:positionHeuristic}. Maximum walking speed can be adjusted but by default it should be around $1.5\frac{m}{s}$ TODO \url{http://en.wikipedia.org/wiki/Preferred_walking_speed}. 
The most important factor of accurate reconstractin are correletions between movements in X,Y and Z axes. Noise in sensors is equally distributed on each axis, so it estimated transaltion should properly keep these movements correlations.

\begin{lstlisting}[caption={Snippet from Android source code position estimation heuristic},label={lst:positionHeuristic},numbers=left,escapeinside={@}{@},float]
    public void onSensorChanged(SensorEvent event) {
            ...
        } else if (event.sensor == mLinearAcceleration) { 
            ...   
            //Filtering out some noise
            newGlobalAcceleration = lowPass(newGlobalAcceleration, currentGlobalAcceleration);
            //Switch linear acceleration from phone local coordinates to global coordinates
            Matrix.multiplyMV(newGlobalAcceleration, 0, invertedRotationMatrix.clone(), 0, newGlobalAcceleration, 0);

            double distance = getLength(currentGlobalAcceleration);
            long currentTimeMillis = System.currentTimeMillis();
            //Decide state of the device. Distance is in m/s^2
            if (distance > 0.5 && deviceState == State.IDLE) {
                if (currentTimeMillis - movingEndTime > 300) {
                    deviceState = State.MOVING;
                    currentGlobalVelocity = {0,0,0};
                }
            } else if (distance < 0.1 && deviceState == State.MOVING) {
                if (currentTimeMillis - movingStartTime > 350) {
                    deviceState = State.IDLE;
                }
            }

            if (deviceState == State.MOVING) {
                //Update current device velocity
                currentGlobalVelocity += currentGlobalAcceleration * dT;
                
                double velocity = getLength(currentGlobalVelocity);
                //Check and adjust current velocity. People walk with avarage speed 1.5\frac{m}{s}
                if (velocity > WALKING_MAX_VELOCITY) {
                    currentGlobalVelocity /= velocity / WALKING_MAX_VELOCITY;
                }
                
                //Update device relative position, s = V0 * t + a * t^2/2;
                currentRelativePosition += currentGlobalVelocity * dT + currentGlobalAcceleration * dT * dT / 2;                 
            }
            ...
    }   
\end{lstlisting}

\subsubsection{Custom Sensor Data File format}
As mentiond earlier each photo sensor information are stored in separe specific file. By default following informations are stored for each captured image:
\begin{itemize}
\item \textbf{ID}, which indicates order of the photos
\item \textbf{Path}, which relative path to corresponding image file
\item \textbf{Euler Angles - pitch, roll, azimuth}
\item \textbf{Relative position coordinate}
\end{itemize} 
All of this information are stored in a list and when the user is done with taking pictures all informations are saved into JSON file named sensor.txt. Sample file can be found in Additional Materials[TODO].\pagebreak

\section{"Enhanced 3D Reconstructer" - OSX CMake based project}
In order to evaluate algorithms there was a need to prepare comfortable project environment, which would allow multiple datasets numerical and visual comparisons. CMake was choosen in order to make simpler build and compilation process. Whole code architecture was developed and tested on Apple MacbookAir with OSX 10.9 Mavericks. In order to compile this project following things need to be installed first:
\begin{itemize}
\item CMake 2.8
\item OpenCV 2.4.10
\item Point Cloud Library (PCL) 1.7
\item Boost 1.55
\item cvsba \url(http://www.uco.es/investiga/grupos/ava/node/39) [TODO reference move]
\end{itemize}
\subsection{User Interface}
There two targets defined in CMake file:
\begin{enumerate}
\item \textbf{Test efficiency}, which was used to evaluate pair reconstruction methods, draw epipolar lines in images and calculate samson error distances
\item \textbf{Test reconsturction}, which was used to evaluate proposed initialization pair reconstruction methods with different pose estimation methods
\end{enumerate}
\subsubsection{Test Efficiency}
There is no graphical interface for reconstruction parameters configuration. All needed parameters are configured from command-line interface. Following parameters has to be configured for this target:
\begin{enumerate}
\item \textbf{Enhanced Photo Data folder path}
\item \textbf{Initial SIFT features number}
\end{enumerate}
Calculated execution times and execution time are printed to the console output.
\subsubsection{Test reconstruction}
There is no graphical interface for reconstruction parameters configuration either. All neccessary parameters have to be configured from command-line interface. Following variables needs to be configured:
\begin{enumerate}
\item \textbf{Enhanced Photo Data folder path}
\item \textbf{Initial SIFT features number}
\item \textbf{Init pair reconstruct method}
\begin{itemize}
\item Standard 8-point algorithm
\item Enhanced 8-point algorithm
\item Standard 5-point algorithm
\item Enhanced 5-point algorithm
\item Alternative 3-point translation estimation
\item None (use existing rotations and translations informations)
\end{itemize}
\item \textbf{Pose estimation method:}
\begin{itemize}
\item Standard OpenCV Pose Estimation
\item Rotation enhanced Pose Estimation 
\item Rotation and translation enhanced Pose Estimation
\item None (use existing rotations and translations)
\end{itemize}
\item \textbf{Whether drop outliers or not}
\item \textbf{Whether use Bundla Adjustment or not}
\end{enumerate}
As output user gets reconstructed models in *.asc extension. Reconstracted model is also visible in PCL Visualizer integrated into the code. User can navigate through model with mouse and 'F' key, which centers camera view on currently selected point.
\subsection{Important Implementation Aspects}
Most of necessery algorithms were already implemented in OpenCV. Also opensource project with 5-point algorithm implementation was used. It source code can be found here: [TODO reference].
\subsubsection{Rotation matrix generation}
To parse Android generated Sensor data file Boost JsonParser was used. As mentioned earlier Android saves decomposed Euler Angles. In order to properly multiply this angles to acquire proper rotation matrix following code inspired on MathWorld Wolfram [TODO \url{http://mathworld.wolfram.com/EulerAngles.html}]
\begin{lstlisting}
Mat getRotation3DMatrix(double pitch, double azimuth, double roll) {
    Mat D = (Mat_<T>(3, 3) <<
            cos(roll), -sin(roll), 0,
            sin(roll), cos(roll), 0,
            0, 0, 1);

    Mat C = (Mat_<T>(3, 3) <<
            cos(azimuth), 0, -sin(azimuth),
            0, 1, 0,
            sin(azimuth), 0, cos(azimuth));
    Mat B = (Mat_<T>(3, 3) <<
            1, 0, 0,
            0, cos(pitch), -sin(pitch),
            0, sin(pitch), cos(pitch));
    //Important
    return B * C * D;
}
\end{lstlisting}
\subsubsection{Enhancing epipolar equations}
As we could observe in \ref{sec:EpipolarEquation} in order to enhance initial pair reconstruction we can use the same algorithms as standard ones, but with specially conditioned point correspondences. Especially standard 8-point algorithm is already available in OpenCV library. Implemented enhanced 8-point version can be found in 'Multiview.cpp' file, but it main difference from standard implementation is that uses specially modified input points sets, which are conditioned and first one is additionaly rotated with Initial Rotation Matrix. In OpenCV it can be done  with:
\begin{lstlisting}
    ...
    undistortPoints(points1Exp, points1Exp, K, distCoeffs, rotDiffGlobal);
    undistortPoints(points2Exp, points2Exp, K, distCoeffs);
    ...
\end{lstlisting}
where rotDiffGlobal is relative rotation matrix between 2 cameras. Estimated Fundamental matrix needs to be transformed to essential matrix for further decomposition. In order to choose proper matrix decomposition followin code is used:
\begin{lstlisting}
void chooseProperMatrixFromEnhanced(Mat &dRx, Mat &dR1x, Mat &TdRExp, Mat &dR, Mat &T) {
    dR = dRx;
    if (decideProperMatrix(dRx, 0.05)) {
        dR = constraintMatrix(dRx);
    } else if (decideProperMatrix(dR1x, 0.05)) {
        dR = constraintMatrix(dR1x);
    } else if (decideProperMatrix(-dRx, 0.05)) {
        dR = constraintMatrix(-dRx);
    } else if (decideProperMatrix(-dR1x, 0.05)) {
        dR = constraintMatrix(-dR1x);
    }

    Mat skewT = TdRExp * dR.inv();
    cout << "skewT" << skewT << endl;

    Mat tdecx = Mat(3,1, CV_64FC1);
    tdecx.at<double>(0) = (skewT.at<double>(2,1) - skewT.at<double>(1,2))/2;
    tdecx.at<double>(1) = (skewT.at<double>(0,2) - skewT.at<double>(2,0))/2;
    tdecx.at<double>(2) = (skewT.at<double>(1,0) - skewT.at<double>(0,1))/2;
    T = tdecx;

}

bool decideProperMatrix(Mat dRot, double tolerance){
    double a00 = abs(dRot.at<double>(0,0) - 1);
    double a11 = abs(dRot.at<double>(1,1) - 1);
    double a22 = abs(dRot.at<double>(2,2) - 1);
    if((a00 + a11 + a22)/3< tolerance) {
        return true;
    }else {
        return false;
    }
}
\end{lstlisting}
These lines helps to decide properly constrainted rotation error matrix \ref{eq:rodiguesError} and calculate relative translation between cameras.
\subsubsection{Alternative 3-point translation estimation}
When it known or assumed that acquired rotation data are accurate translation can be calculated with 3-point algorithm. Proposed in \ref{eq:alternative3point} and \ref{eq:translation3point} were implemented. As similar to other Epipolar estimations methods implemented in OpenCV, this one also has ability to properly filter out outliers, when RANSAC algorithm is used. Following listing contains this implementation:
\begin{lstlisting}
    ...
    for (iterationNumber = 0; iterationNumber < niters; iterationNumber++) {

        getSubsety(prev_points_raw, next_points_raw, point1s, point2s, 300, modelPoints);
        Mat t = findTranslation(point1s, point2s, rotDiffGlobal, Kinv);
        Mat F1 = constructFundamentalMatrix(rotDiffGlobal, t, Kinv);

        int goodCount = findInliersy(prev_points_raw, next_points_raw, F1, errors, statuses, reprojThreshold);
        if (goodCount > maxGoodCount) {
            swap(statuses, goodStatuses);
            FEnhanced = F1;
            tEnhanced = t / t.at<double>(2);
            maxGoodCount = goodCount;
            niters = cvRANSACUpdateNumIters(confidence,
                    (double) (count - maxGoodCount) / count, modelPoints, niters);
        }

    }
    ...
    
    
    Mat findTranslation(std::vector<cv::Point2d> &points1, std::vector<cv::Point2d> &points2, Mat &rotDiff, Mat &Kinv) {

    Mat hg1 = Mat::zeros(points1.size(), 3, CV_64FC1);
    Mat hg2 = Mat::zeros(points2.size(), 3, CV_64FC1);
    for (int i = 0; i < points1.size(); i++) {
        hg1.at<double>(i, 0) = points1[i].x;
        hg1.at<double>(i, 1) = points1[i].y;
        hg1.at<double>(i, 2) = 1;
        hg2.at<double>(i, 0) = points2[i].x;
        hg2.at<double>(i, 1) = points2[i].y;
        hg2.at<double>(i, 2) = 1;
    }

    hg1 = hg1 * (rotDiff * Kinv).t();
    hg2 = hg2 * (Kinv).t();

    Mat A = Mat::zeros(hg1.rows, 3, CV_64FC1);
    for (int i = 0; i < hg1.rows; i++) {
        A.at<double>(i, 0) = (hg2.at<double>(i, 2) * hg1.at<double>(i, 1) - hg2.at<double>(i, 1) * hg1.at<double>(i, 2));
        A.at<double>(i, 1) = (hg2.at<double>(i, 0) * hg1.at<double>(i, 2) - hg2.at<double>(i, 2) * hg1.at<double>(i, 0));
        A.at<double>(i, 2) = (hg2.at<double>(i, 1) * hg1.at<double>(i, 0) - hg2.at<double>(i, 0) * hg1.at<double>(i, 1));
    }

    SVD svd1(A);
    Mat tCalc = svd1.vt.row(2);

    //Translation between cameras estimated and Fundamental Matrix from that as well
    Mat T = (tCalc.t());

    return T;

\end{lstlisting}
\subsubsection{Enhancing pose estimation}
OpenCv has already rotation and translation enhanced pose estimation implemented. The only thing that has to be done is to perform conversion of euclidan rotation matrix to its Rodrigues representation. Initial rotation and translation needs to be passed as input variables. Pose estimation methods implementations: 
\begin{lstlisting}
Mutiview::FindPoseEstimation(
        cv::Mat &rvec,
        cv::Mat &t,
        cv::Mat &R,
        cv::Mat &K,
        cv::Mat &distCoeffs,
        std::vector<cv::Point3d> ppcloud,
        std::vector<cv::Point2d> imgPoints,
        vector<int> inliers) 
Mutiview::FindPoseEstimationEnhanced(
        cv::Mat &rvec,
        cv::Mat &t,
        cv::Mat &R,
        cv::Mat &RInit,
        cv::Mat &TInit,
        cv::Mat &K,
        cv::Mat &distCoeffs,
        std::vector<cv::Point3d> ppcloud,
        std::vector<cv::Point2d> imgPoints,
        vector<int> inliers)
\end{lstlisting}
can be found in 'Mutiview.cpp' file.


% ---------------------------------------------------------------------------
%: ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------

